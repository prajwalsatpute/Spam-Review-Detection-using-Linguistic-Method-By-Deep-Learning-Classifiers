{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "be5ce724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin           reviewerName helpful  \\\n",
      "0  A1YS9MDZP93857  0006428320            John Taylor  [0, 0]   \n",
      "1  A3TS466QBAWB9D  0014072149          Silver Pencil  [0, 0]   \n",
      "2  A3BUDYITWUSIS7  0041291905  joyce gabriel cornett  [0, 0]   \n",
      "3  A19K10Z0D2NTZK  0041913574            TexasCowboy  [0, 0]   \n",
      "4  A14X336IB4JD89  0201891859                 dfjm53  [0, 1]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  The portfolio is fine except for the fact that...      3.0   \n",
      "1  If you are a serious violin student on a budge...      5.0   \n",
      "2  This is and excellent edition and perfectly tr...      5.0   \n",
      "3  Perfect for someone who is an opera fan or a w...      5.0   \n",
      "4  How many Nocturnes does it contain? All of the...      1.0   \n",
      "\n",
      "                            summary  unixReviewTime   reviewTime  \n",
      "0                     Parts missing      1394496000  03 11, 2014  \n",
      "1  Perform it with a friend, today!      1370476800   06 6, 2013  \n",
      "2           Vivalldi's Four Seasons      1381708800  10 14, 2013  \n",
      "3   Full score: voice and orchestra      1285200000  09 23, 2010  \n",
      "4      Unable to determine contents      1350432000  10 17, 2012  \n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Musical_Instruments.json.gz')\n",
    "print(df.head(5))\n",
    "df=df.head(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "421db3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'algo - B.ipynb', 'archive.zip', 'birnn.ipynb', 'cnn.ipynb', 'GFG1.csv', 'glove.6B.100d.txt', 'mlp.ipynb', 'reviews_Musical_Instruments.json', 'reviews_Musical_Instruments.json.gz', 'Untitled - Copy.ipynb', 'Untitled - Copy_.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dataset = pd.read_csv(\"GFG1.csv\")\n",
    "dataset=dataset.head(50000)\n",
    "Y=dataset[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aab06140",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['reviewText']\n",
    "y = dataset[\"label\"]\n",
    "y=y.replace((np.inf, -np.inf, np.NaN), False).reset_index(drop=True)\n",
    "\n",
    "def pro(pic):\n",
    "    v=(df.loc[df['reviewText'].str.contains(pic)]) \n",
    "    v=(v.index)\n",
    "    v=(v[0])\n",
    "\n",
    "    return(Y.iloc[[v]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "53ea486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "from numpy import random\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import re\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ae1d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    #text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "X = X.map(lambda a: clean_text(a))\n",
    "def text_to_wordlist(text):    \n",
    "    #Remove Special Characters\n",
    "    text = re.sub(r'[^a-z\\d ]', \" \", text)\n",
    "    text = re.sub(r'\\d+', '_num_', text)    \n",
    "    return(text)\n",
    "X = X.map(lambda a: text_to_wordlist(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c387f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, y_train, y_test = train_test_split(X,y,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5458b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 76540\n",
      "Longest comment size: 3571\n",
      "Average comment size: 99.84224\n",
      "Stdev of comment size: 130.81840356686212\n",
      "Max comment size: 492\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=' ',char_level=False)\n",
    "tokenizer.fit_on_texts(X)\n",
    "x_train = tokenizer.texts_to_sequences(X)\n",
    "x_test = tokenizer.texts_to_sequences(X)\n",
    "word_index = tokenizer.word_index\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "vocab_size = len(word_index)\n",
    "y_train=y\n",
    "print('Vocab size: {}'.format(vocab_size))\n",
    "longest = max(len(seq) for seq in X)\n",
    "print(\"Longest comment size: {}\".format(longest))\n",
    "average = np.mean([len(seq) for seq in X])\n",
    "y_test=y\n",
    "print(\"Average comment size: {}\".format(average))\n",
    "stdev = np.std([len(seq) for seq in X])\n",
    "print(\"Stdev of comment size: {}\".format(stdev))\n",
    "max_len = int(average + stdev * 3)\n",
    "print('Max comment size: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "332e6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 492)\n",
      "x_test shape: (50000, 492)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#!pip install keras==2.3.1\n",
    "processed_post_x_train = pad_sequences(x_train, maxlen=max_len, padding='post', truncating='post')\n",
    "processed_post_x_test = pad_sequences(x_test, maxlen=max_len, padding='post', truncating='post')\n",
    "processed_x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "processed_x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', processed_x_train.shape)\n",
    "print('x_test shape:', processed_x_test.shape)\n",
    "import tensorflow.keras.backend\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import GRU, Dense, Conv1D, MaxPooling1D\n",
    "from keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization, LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Nadam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9144b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_dim = 100\n",
    "k = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        k += 1\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "955ed434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 492, 100)          7654100   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 492, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 164, 128)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 7,703,509\n",
      "Trainable params: 7,703,253\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True))\n",
    "\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))#,kernel_regularizer=l2(0.01),bias_regularizer=l2(0.01)))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(64, activation='relu',kernel_regularizer=l2(0.17),bias_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='sigmoid',kernel_regularizer=l2(0.19),bias_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "#model.add(Dense(16, kernel_regularizer=l2(0.001), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6c8886d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 50000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 377s 8ms/step - loss: 1.0807 - accuracy: 0.9469 - val_loss: 0.2053 - val_accuracy: 0.9479\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(processed_x_train,y_train,validation_data=(processed_x_test,y_test),epochs=1,batch_size=32,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d5d385be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.95      0.97     50000\n",
      "\n",
      "    accuracy                           0.95     50000\n",
      "   macro avg       0.50      0.47      0.49     50000\n",
      "weighted avg       1.00      0.95      0.97     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res=(model.predict_classes(processed_x_test))\n",
    "#res=np.argmax(res)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(res,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913946ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b29eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pymysql as mdb \n",
    "def update(result, ids):\n",
    "    mydb = mdb.connect(\n",
    "      host=\"127.0.0.1\",\n",
    "      user=\"root\",\n",
    "      passwd=\"\",\n",
    "      database=\"crm\"\n",
    "    )\n",
    "    mycursor = mydb.cursor()\n",
    "    sql = \"UPDATE spam SET predict = %s WHERE id = %s\"\n",
    "    val = (result, ids)\n",
    "    mycursor.execute(sql, val)\n",
    "    mydb.commit()\n",
    "    mydb.close()\n",
    "while True:\n",
    "    mydb = mdb.connect(\n",
    "          host=\"127.0.0.1\",\n",
    "          user=\"root\",\n",
    "          passwd=\"\",\n",
    "          database=\"crm\"\n",
    "        )\n",
    "    mycursor = mydb.cursor()\n",
    "    sql = \"SELECT review,predict,id FROM spam\"\n",
    "    mycursor.execute(sql)\n",
    "   \n",
    "    myresult = mycursor.fetchall()\n",
    "    mydb.close()\n",
    "    for x in myresult:\n",
    "      ids= str(x[2])\n",
    "      pic=str(x[0])\n",
    "      fl=str(x[1])\n",
    "      \n",
    "      if fl=='':\n",
    "        print('prediction')\n",
    "        a=[]\n",
    "        pic=str(pro(pic))\n",
    "        print(pic)\n",
    "        print('----------------------')\n",
    "        print(pic[0:14])\n",
    "        c=str(pic[0:14])\n",
    "        print(c.split(' '))\n",
    "        try:\n",
    "            a,b,c,d,e=(c.split(' '))\n",
    "        except:\n",
    "            a,b,c,d,e,f=(c.split(' '))\n",
    "        pic=int(e[0])\n",
    "        print(e)\n",
    "        print('----------------------')\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            pic1=text_to_wordlist(pic)\n",
    "            pic1=tokenizer.texts_to_sequences(pic)\n",
    "\n",
    "            pic1=pad_sequences(pic, maxlen=max_len, padding='post', truncating='post')\n",
    "            #print(pic)\n",
    "            a.append(pic)\n",
    "            res=(model.predict_classes(a))\n",
    "            res=np.argmax(res)\n",
    "            res=pic\n",
    "            \n",
    "        except:\n",
    "            res=pic\n",
    "            vb=0\n",
    "        res=str(res)\n",
    "        print(res)\n",
    "        \n",
    "        update(res, ids)    \n",
    "        \n",
    "\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5e2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840e527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497a214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
